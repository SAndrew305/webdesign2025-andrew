<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Running Locally</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 20px;
    }
    h1, h2 {
      color: #333;
    }
    pre {
      background-color: #f4f4f4;
      padding: 10px;
      border: 1px solid #ccc;
      font-family: monospace;
    }
    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin-top: 10px;
    }
    ul, ol {
      list-style-type: disc;
      padding-left: 20px;
    }
    li {
      margin-bottom: 5px;
    }
    p {
      margin-bottom: 10px;
    }
  </style>
</head>
<body>

<h1>Running Locally</h1>

<p>Surprisingly, getting an LLM on your system is much easier than you'd think it would be. You don't need a $50k computer to just do this stuff, so I'm going to assume you're using a traditional computer or a Mac. Be aware that the things you'll be able to do depend on your hardware, so be mindful of anything you might be planning to do, like if you wanted to run something heavy like Text-to-video models, you'll need a specific type of GPU.</p>

<p>Oh, right. I should probably cover this briefly.. Basically, you're going to run this LLM on a part of your computer called "baremetal." Baremetal consists of the three most important parts of a computer, and if you intend on using a specific model or having it run fast, you're also going to need a GPU. Here is a quick summary of how baremetal + a GPU works from yours truly:</p>

<h2>Baremetal</h2>
<ul>
  <li><strong>Processor (CPU)</strong> - The brains of the computer, this handles everything from running the operating system (like Windows), video game physics (if not rerouted to the GPU), and a bit of your model.</li>
  <li><strong>Memory (RAM)</strong> - Short-term Storage. Faster than a hard-drive, RAM is perfect for storing and loading your model onto to have it run fast. (<strong>BE WARNED!</strong>: Due to extremely high demand for AI data centers, RAM price has skyrocketed about 2 to 4x it's current market value, due to major RAM manufacturers shutting down production on consumer grade components to focus exclusively on selling RAM to data centers.)</li>
  <li><strong>Hard Drive (HDD or SSD)</strong> - Long-term Storage. Slower than memory, hard drives are excellent for storing all your models when not loaded to RAM.</li>
</ul>

<h2>Nice-to-haves</h2>
<ul>
  <li><strong>Graphics Card (GPU)</strong> - Basically a CPU, but dedicated solely to processing intense mathematical calculations, mainly processing video games, but has become a very popular and attractive option for running AI due to having features making it easier and faster to run AI.</li>
</ul>

<p>I didn't get everything, just the things you'll need to get working. You can find a better guide on the internet. Anyway- back to the main discussion:</p>

<h3><u>Things you need - Windows/Linux</u></h3>
<ul>
  <li><strong>Processor</strong>: A 6-Core processor with a base speed of around 3 GHz, with AVX support (Anything made in the last 5-7 years should do nicely).</li>
  <li><strong>Memory</strong>: This very heavily depends on the model you're using, but assuming you intend to run 3-4b Models, 16GB should do nicely. I HEAVILY recommend using DDR4 or DDR5, because of it's speed for storing and running LLM's.</li>
  <li><strong>Storage</strong>: Whether you have an HDD or and SSD doesn't matter, what matters is that you have at least 10-25GB free to store the software and the model.</li>
  <li><strong>Graphics Card (optional)</strong>: If you're not doing anything heavy, integrated graphics should suffice, but if you intend to use a GPU, it is advised to use an NVIDIA GPU, because even older cards like the GTX 960 (released in 2015) have something called CUDA, which allows accelerated performance for running LLMs.</li>
</ul>

<h3><u>Things you need - Mac</u></h3>
<ol>
  <li><strong>Processor</strong>: Any Quad-Core Intel i5 made after 2017, or any Apple M-Series processor.</li>
  <li><strong>Memory</strong>: Again, heavily depends on model used, 8GB for lower end models, 16GB for 3-4b, and everything else, get more RAM.</li>
  <li><strong>Storage</strong>: 10-25GB free.</li>
</ol>

<p>Check to make sure you fit along these specs, but know that these are soft-requirements, not hard-requirements, meaning that even if you have lower end hardware, you might still be able to run at the cost of speed and model accuracy.</p>

<p>The system I will be using for this demonstration has the following specs;</p>
<ul>
  <li><strong>Processor</strong>: Intel Core i5 13500 @ 3.6GHz 6 Core 12 Thread</li>
  <li><strong>Memory</strong>: 16GB DDR5-4800 (not the best, but make use of what you got)</li>
  <li><strong>Graphics</strong>: Integrated (Intel UHD 660)</li>
</ul>

<h2>Setting Up - Prerequisites</h2>
<ol>
  <li><a href="https://lmstudio.ai">Download the LM Studio installer</a></li>
  <li>Run the installer. It should be pretty straightforward.</li>
  <li>You should have something like this; <img src="Pasted image 20260105090655.png"></img></li>
  <p>This is the layout for users after running LM Studio for the first time. Usually LM Studio will start by installing a model called gpt-oss 20b. We want to avoid that, as we will be installing our own model.</p>
</ol>

<h2>Running the model</h2>
<ol>
  <li>Install a model like Ministral 3 3B. Make sure you still have enough available memory to run the model.</li>
  <li>Load the model;<img src="20260105-1715-32.7796600.gif"></img></li>
  <li>Type in a prompt of your choosing and watch the magic unfold;<img src="Recording 2026-01-05 092111.gif"></img></li>
  <li>And just like that, you have run an LLM on your system locally, free of charge, without the giant tech overlords watching.</li>
</ol>

<p>This was not so hard, was it? Keep in mind this is for LLM's, not text to video or text to image models. You need a beefy GPU for those, and I do not have that.</p>

</body>
</html>
